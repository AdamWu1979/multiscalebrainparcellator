#!/usr/bin/env python
# -*-coding:Latin-1 -*

# Copyright (C) 2017-2019, Brain Communication Pathways Sinergia Consortium, Switzerland
# All rights reserved.
#
#  This software is distributed under the open-source license Modified BSD.
"""
Syntax :
%
Description
%
Input Parameters:
%
Output Parameters:
%
%
Related references:

See also:
__________________________________________________
Authors: SÃ©bastien Tourbier
Radiology Department
CHUV, Lausanne
Created on 2018-11-19
Version $0.1

======================= Importing Libraries ==============================
"""
import sys
import os
import shutil
from os import path as op
import subprocess
from glob import glob

import multiprocessing

from cmp.multiscalebrainparcellator.info import __version__
import cmp.multiscalebrainparcellator.project
import cmp.multiscalebrainparcellator.parser as parser

from cmtklib.util import bcolors

# Remove warnings visible whenever you import scipy (or another package) that was compiled against an older numpy than is installed.
import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

# try:
#     import nibabel as ni
# except ImportError:
#     print("Module nibabel not found. Please install it")
#     sys.exit(1)
#
# try:
#     import numpy as np
# except ImportError:
#     print("Numpy module not found. Please install it")
#     sys.exit(1)
#
# try:
#     from nipype.interfaces.base import traits, BaseInterfaceInputSpec, TraitedSpec, BaseInterface, Directory, File, OutputMultiPath
#     from nipype.utils.logger import logging
#     iflogger = logging.getLogger('interface')
# except ImportError:
#     print("Nipype module not found. Please install it")
#     sys.exit(1)
#
# try:
#     from cmtklib.parcellation import create_annot_label, create_roi, create_wm_mask,  crop_and_move_datasets, generate_WM_and_GM_mask, crop_and_move_WM_and_GM, get_parcellation
# except ImportError:
#     print("CMTKlib module not found. Please install it")
#     sys.exit(1)

"""
====================End of Importing Libraries ===========================

"""

def run(command, env={}):
    merged_env = os.environ
    merged_env.update(env)
    process = subprocess.Popen(command, stdout=subprocess.PIPE,
                               stderr=subprocess.STDOUT, shell=True,
                               env=merged_env)
    while True:
        line = process.stdout.readline()
        line = str(line)[:-1]
        print(line)
        if line == '' and process.poll() != None:
            break
    if process.returncode != 0:
        raise Exception("Non zero return code: %d"%process.returncode)


if __name__ == '__main__':
    """Entry point"""
    pars = parser.get()
    args = pars.parse_args()

    print('BIDS dataset: {}'.format(args.bids_dir))

    #Make Freesurfer happy with the ficense
    #Multiscale brain parcellator uses the FS_LICENSE environment to point to the license
    # It supposes that the license is stored as /bids_dir/code/license.txt
    
    dest = '{}/code/license.txt'.format(args.bids_dir)
    if args.fs_license:
        if not shutil._samefile(args.fs_license,dest):    
            if os.access(dest,os.F_OK):
                os.remove(dest)
                print('MAIN WARNING: Freesurfer license.txt has been replaced in bids_dir/code folder')
            if not os.access('{}/code'.format(args.bids_dir),os.F_OK):
                os.mkdir('{}/code'.format(args.bids_dir))
            shutil.copyfile(args.fs_license,dest)
            print('MAIN INFO: Freesurfer license provided as input will be used'.format(dest))       
        else:
            print('MAIN WARNING: Freesurfer license specified by --fs_license () is the same as the destination license and will not be copied'.format(dest))              
    else:      
        if not os.access(dest,os.F_OK):
            if os.environ['FREESURFER_HOME']:
                src = '{}/license.txt'.format(os.environ['FREESURFER_HOME'])
                if os.access(src,os.F_OK):
                    if os.access(dest,os.F_OK):
                        os.remove(dest)
                        print('MAIN WARNING: Freesurfer license.txt has been replaced in bids_dir/code folder')
                    if not os.access('{}/code'.format(args.bids_dir),os.F_OK):
                        os.mkdir('{}/code'.format(args.bids_dir))
                    shutil.copyfile(src,dest)
                    print('MAIN INFO: Freesurfer license located in $FREESURFER_HOME will be used'.format(dest))

            else:
                print('MAIN ERROR: Freesurfer license not found in directory pointed by $FREESURFER_HOME. Freesurfer license has to be defined using the --fs_license input flag.')
                sys.exit(2)
        else:
            print('MAIN INFO: {} Freesurfer license found will be used'.format(dest))

    # Keep one core for light processes
    max_number_of_cores = multiprocessing.cpu_count()-1

    #Setup number of cores to be used by nipype multiproc plugin
    multiproc_maxprocs = int(args.multiproc_number_of_cores)
    
    if multiproc_maxprocs != None:
        if multiproc_maxprocs > max_number_of_cores:
            print('MAIN WARNING: the specified number of cores ({}) exceeds the number of available cores ({})'.format(args.multiproc_number_of_cores,max_number_of_cores))
            print('MAIN INFO: Maximal number of cores set to the maximal number of available cores ({})'.format(max_number_of_cores))
            multiproc_maxprocs = max_number_of_cores
        elif multiproc_maxprocs <= 0:
            print('MAIN WARNING: the specified number of cores ({}) should be greater to 0'.format(args.multiproc_number_of_cores))
            print('MAIN INFO: Use 1/{} core'.format(max_number_of_cores))
            multiproc_maxprocs = 1
        else:
            print('MAIN INFO: Use {}/{} cores'.format(multiproc_maxprocs,max_number_of_cores))
    else:
        print('MAIN INFO: Use 1/{} core '.format(max_number_of_cores))
        multiproc_maxprocs = 1  

    #Setup number of subjects to be processed in parallel
    parallel_number_of_subjects = int(args.number_of_participants_processed_in_parallel)
    
    if parallel_number_of_subjects != None:
        if parallel_number_of_subjects > max_number_of_cores:
            print('MAIN WARNING: the specified number of subjects to be processed in parallel ({}) exceeds the number of available cores ({})'.format(args.number_of_participants_processed_in_parallel,max_number_of_cores))
            print('MAIN INFO: Maximal number of cores set to the maximal number of available cores ({})'.format(max_number_of_cores))
            parallel_number_of_subjects = max_number_of_cores
        elif parallel_number_of_subjects <= 0:
            print('MAIN WARNING: the specified number of subjects to be processed in parallel ({}) should be greater to 0'.format(args.number_of_participants_processed_in_parallel))
            print('MAIN INFO: sequential processing (use one core)')
            parallel_number_of_subjects = 1
        else:
            print('MAIN INFO: Process {} subjects in parallel (Total of cores available: {})'.format(parallel_number_of_subjects,max_number_of_cores))
    else:
        print('MAIN INFO: sequential processing (use one core)')
        parallel_number_of_subjects = 1  

    #Setup number of cores to be used by nipype multiproc plugin
    fs_maxprocs = int(args.fs_number_of_cores)
    
    if fs_maxprocs != None:
        if fs_maxprocs > max_number_of_cores:
            print('MAIN WARNING: the specified number of cores ({}) used by FreeSurfer exceeds the number of available cores ({})'.format(args.fs_number_of_cores,max_number_of_cores))
            print('MAIN INFO: Maximal number of cores set to the maximal number of available cores ({})'.format(max_number_of_cores))
            fs_maxprocs = max_number_of_cores
        elif fs_maxprocs <= 0:
            print('MAIN WARNING: the specified number of cores ({}) used by FreeSurfer should be greater to 0'.format(args.fs_number_of_cores))
            print('MAIN INFO: FreeSurfer sequential execution (use one core)')
            fs_maxprocs = 1
        else:
            print('MAIN INFO: Freesurfer uses {}/{} cores'.format(fs_maxprocs,max_number_of_cores))
    else:
        print('MAIN INFO: FreeSurfer sequential execution (use one core)')
        fs_maxprocs = 1  

    #Make sure that the total number of cores used does not exceed the total number of available cores
    total_number_of_cores_used = multiproc_maxprocs * (parallel_number_of_subjects + fs_maxprocs)
    if total_number_of_cores_used > max_number_of_cores:
        print('MAIN WARNING: Total number of cores used (Subjects in parallel: {}, Pipeline processes in parallel: {}, Freesurfer: {} , Total: {}) is greater than the number of available cores ({})'.format(parallel_number_of_subjects,multiproc_maxprocs,fs_maxprocs,total_number_of_cores_used, max_number_of_cores)) 
        multiproc_maxprocs = 1
        fs_maxprocs = 1
        parallel_number_of_subjects = max_number_of_cores
        print('MAIN INFO: Processing will be ONLY parallelized at the subject level using {} cores.'.format(parallel_number_of_subjects))
    
    #Setup the isotropic resolution

    resolution = args.isotropic_resolution
    if (resolution <= 0) or (resolution is None):
        resolution=1.0
        print('MAIN INFO: Default isotropic resolution for resampling: {}'.format(resolution))
    else:
        resolution = float(resolution)
        print('MAIN INFO: Custom isotropic resolution for resampling: {}'.format(resolution))

    # Run or not the bids validator
    if not args.skip_bids_validator:
        run('bids-validator %s'%args.bids_dir)

    subjects_to_analyze = []
    # only for a subset of subjects
    if args.participant_label:
        subjects_to_analyze = args.participant_label
    # for all subjects
    else:
        subject_dirs = glob(os.path.join(args.bids_dir, "sub-*"))
        subjects_to_analyze = [subject_dir.split("-")[-1] for subject_dir in subject_dirs]

    subjects = ['sub-{}'.format(label) for label in subjects_to_analyze]

    #Derivatives directory creation if it does not exist
    toolbox_derivatives_dir = os.path.join(args.output_dir , "cmp")
    if not os.access(toolbox_derivatives_dir,os.F_OK):
        os.mkdir(toolbox_derivatives_dir)

    #Check if freesurfer derivatives is already present
    freesurfer_derivatives_dir = os.path.join(args.output_dir , "freesurfer")
    if os.access(freesurfer_derivatives_dir,os.F_OK):
        # Compute freesurfer derivatives directory size
        total_size = 0
        for path, dirs, files in os.walk(freesurfer_derivatives_dir):
            for f in files:
                fp = os.path.join(path, f)
                total_size += os.path.getsize(fp)
        print("MAIN INFO: Existing freesurfer derivatives found (size: {})".format(total_size))
    else:
        print("MAIN INFO: Existing freesurfer derivatives NOT found")

    # Check subject/session structure og the BIDS dataset
    # Create a new list of (subject,session) pairs if sessions are used

    use_session_structure = False
    subjects_sessions = []

    for subject in subjects:
        # Check if multiple session (sub-XX/ses-YY/anat/... structure or sub-XX/anat.. structure?)
        subject_session_dirs = glob(os.path.join(args.bids_dir, subject, "ses-*"))
        subject_sessions = ['ses-{}'.format(subject_session_dir.split("-")[-1]) for subject_session_dir in subject_session_dirs]
        if len(subject_sessions) > 0: #Session structure
            use_session_structure = True
            for subject_session in subject_sessions:
                subjects_sessions.append([subject,subject_session])
    print('INFO: Session structured dataset: {}'.format(use_session_structure))

    if not use_session_structure:
        print("MAIN INFO: Subjects to be analyzed: {}".format(subjects))
    else:
        print("MAIN INFO: Subjects/Sessions to be analyzed: {}".format(subjects_sessions))

    # running participant level
    if args.analysis_level == "participant":

        processes = []

        if not use_session_structure:
            for subject in subjects:
                while len(processes) == parallel_number_of_subjects:
                    cmp.multiscalebrainparcellator.project.manage_procs(processes)

                project_info, config_file = cmp.multiscalebrainparcellator.project.create_configuration_file_participant_level(args.bids_dir,args.output_dir,subjects,subject,'',resolution,args.thalamic_nuclei,args.hippocampal_subfields,args.brainstem_structures,multiproc_number_of_cores=multiproc_maxprocs,fs_number_of_cores=fs_maxprocs)
                proc = cmp.multiscalebrainparcellator.project.participant_level_process(project_info,config_file)
                processes.append(proc)

                while len(processes) > 0:
                    cmp.multiscalebrainparcellator.project.manage_procs(processes)

                print("MAIN INFO: Processing with the Multi-scale Brain Parcellator BIDS App finished!")


        else:
            for subject,subject_session in subjects_sessions:
                while len(processes) == parallel_number_of_subjects:
                    cmp.multiscalebrainparcellator.project.manage_procs(processes)

                project_info, config_file = cmp.multiscalebrainparcellator.project.create_configuration_file_participant_level(args.bids_dir,args.output_dir,subjects,subject,subject_session,resolution,args.thalamic_nuclei,args.hippocampal_subfields,args.brainstem_structures,multiproc_number_of_cores=multiproc_maxprocs,fs_number_of_cores=fs_maxprocs)
                proc = cmp.multiscalebrainparcellator.project.participant_level_process(project_info,config_file)
                processes.append(proc)

                while len(processes) > 0:
                    cmp.multiscalebrainparcellator.project.manage_procs(processes)

                print("MAIN INFO: Processing with the Multi-scale Brain Parcellator BIDS App finished!")


    # running group level; ultimately it will compute average connectivity matrices
    elif args.analysis_level == "group":
        print('MAIN ERROR: Sorry but this BIDS App has only the subject level processing pipeline implemented that can be run with --participant' )
        sys.exit(1)
