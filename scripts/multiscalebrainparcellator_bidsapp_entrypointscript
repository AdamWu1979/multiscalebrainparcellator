#!/usr/bin/env python
# -*-coding:Latin-1 -*

# Copyright (C) 2017-2019, Brain Communication Pathways Sinergia Consortium, Switzerland
# All rights reserved.
#
#  This software is distributed under the open-source license Modified BSD.
"""
Syntax :
%
Description
%
Input Parameters:
%
Output Parameters:
%
%
Related references:

See also:
__________________________________________________
Authors: SÃ©bastien Tourbier
Radiology Department
CHUV, Lausanne
Created on 2018-11-19
Version $0.1

======================= Importing Libraries ==============================
"""
import sys
import os
from os import path as op
import subprocess
from glob import glob

import multiprocessing

from cmp.multiscalebrainparcellator.info import __version__
import cmp.multiscalebrainparcellator.project
import cmp.multiscalebrainparcellator.parser as parser

# Remove warnings visible whenever you import scipy (or another package) that was compiled against an older numpy than is installed.
import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

# try:
#     import nibabel as ni
# except ImportError:
#     print("Module nibabel not found. Please install it")
#     sys.exit(1)
#
# try:
#     import numpy as np
# except ImportError:
#     print("Numpy module not found. Please install it")
#     sys.exit(1)
#
# try:
#     from nipype.interfaces.base import traits, BaseInterfaceInputSpec, TraitedSpec, BaseInterface, Directory, File, OutputMultiPath
#     from nipype.utils.logger import logging
#     iflogger = logging.getLogger('interface')
# except ImportError:
#     print("Nipype module not found. Please install it")
#     sys.exit(1)
#
# try:
#     from cmtklib.parcellation import create_annot_label, create_roi, create_wm_mask,  crop_and_move_datasets, generate_WM_and_GM_mask, crop_and_move_WM_and_GM, get_parcellation
# except ImportError:
#     print("CMTKlib module not found. Please install it")
#     sys.exit(1)

"""
====================End of Importing Libraries ===========================

"""

def run(command, env={}):
    merged_env = os.environ
    merged_env.update(env)
    process = subprocess.Popen(command, stdout=subprocess.PIPE,
                               stderr=subprocess.STDOUT, shell=True,
                               env=merged_env)
    while True:
        line = process.stdout.readline()
        line = str(line)[:-1]
        print(line)
        if line == '' and process.poll() != None:
            break
    if process.returncode != 0:
        raise Exception("Non zero return code: %d"%process.returncode)


if __name__ == '__main__':
    """Entry point"""
    pars = parser.get()
    args = pars.parse_args()

    print('BIDS dataset: {}'.format(args.bids_dir))

    #Setup number of cores to use
    maxprocs = args.number_of_cores
    max_number_of_cores = multiprocessing.cpu_count()
    if maxprocs > max_number_of_cores:
        print('WARNING: the specified number of cores ({}) exceeds the number of available cores ({})'.format(args.number_of_cores,max_number_of_cores))
        print('INFO: Maximal number of cores set to the maximal number of available cores ({})'.format(max_number_of_cores))
        maxprocs = max_number_of_cores
    elif maxprocs <= 0:
        print('WARNING: the specified number of cores ({}) should be greater to 0'.format(args.number_of_cores))
        print('INFO: Maximal number of cores set to the maximal number of available cores ({})'.format(max_number_of_cores))
        maxprocs = max_number_of_cores
    else:
        print('INFO: Maximal number of processing cores set to {}'.format(maxprocs))

    #Setup the isotropic resolution

    resolution = args.isotropic_resolution
    if (resolution <= 0) or (resolution is None):
        resolution=1.0
        print('Default isotropic resolution for resampling: {}'.format(resolution))
    else:
        resolution = float(resolution)
        print('Custom isotropic resolution for resampling: {}'.format(resolution))

    # Run or not the bids validator
    if not args.skip_bids_validator:
        run('bids-validator %s'%args.bids_dir)

    subjects_to_analyze = []
    # only for a subset of subjects
    if args.participant_label:
        subjects_to_analyze = args.participant_label
    # for all subjects
    else:
        subject_dirs = glob(os.path.join(args.bids_dir, "sub-*"))
        subjects_to_analyze = [subject_dir.split("-")[-1] for subject_dir in subject_dirs]

    subjects = ['sub-{}'.format(label) for label in subjects_to_analyze]

    #Derivatives directory creation if it does not exist
    toolbox_derivatives_dir = os.path.join(args.output_dir , "cmp")
    if not os.path.isdir(toolbox_derivatives_dir):
        os.mkdir(toolbox_derivatives_dir)

    # Check subject/session structure og the BIDS dataset
    # Create a new list of (subject,session) pairs if sessions are used

    use_session_structure = False
    subjects_sessions = []

    for subject in subjects:
        # Check if multiple session (sub-XX/ses-YY/anat/... structure or sub-XX/anat.. structure?)
        subject_session_dirs = glob(os.path.join(args.bids_dir, subject, "ses-*"))
        subject_sessions = ['ses-{}'.format(subject_session_dir.split("-")[-1]) for subject_session_dir in subject_session_dirs]
        if len(subject_sessions) > 0: #Session structure
            use_session_structure = True
            for subject_session in subject_sessions:
                subjects_sessions.append([subject,subject_session])
    print('Session structured dataset: {}'.format(use_session_structure))

    if not use_session_structure:
        print("Subjects to be analyzed: {}".format(subjects))
    else:
        print("Subjects/Sessions to be analyzed: {}".format(subjects_sessions))

    # running participant level
    if args.analysis_level == "participant":

        processes = []

        if not use_session_structure:
            for subject in subjects:
                while len(processes) == maxprocs:
                    cmp.multiscalebrainparcellator.project.manage_procs(processes)

                project_info, config_file = cmp.multiscalebrainparcellator.project.create_configuration_file_participant_level(args.bids_dir,args.output_dir,subjects,subject,'',resolution,args.thalamic_nuclei,args.hippocampal_subfields,args.brainstem_structures)
                proc = cmp.multiscalebrainparcellator.project.participant_level_process(project_info,config_file)
                processes.append(proc)

                while len(processes) > 0:
                    cmp.multiscalebrainparcellator.project.manage_procs(processes)

                print("Processing with the Multi-scale Brain Parcellator BIDS App finished!")


        else:
            for subject,subject_session in subjects_sessions:
                while len(processes) == maxprocs:
                    cmp.multiscalebrainparcellator.project.manage_procs(processes)

                project_info, config_file = cmp.multiscalebrainparcellator.project.create_configuration_file_participant_level(args.bids_dir,args.output_dir,subjects,subject,subject_session,resolution,args.thalamic_nuclei,args.hippocampal_subfields,args.brainstem_structures)
                proc = cmp.multiscalebrainparcellator.project.participant_level_process(project_info,config_file)
                processes.append(proc)

                while len(processes) > 0:
                    cmp.multiscalebrainparcellator.project.manage_procs(processes)

                print("Processing with the Multi-scale Brain Parcellator BIDS App finished!")


    # running group level; ultimately it will compute average connectivity matrices
    elif args.analysis_level == "group":
        print('Error: Sorry but this BIDS App has only the subject level processing pipeline implemented that can be run with --participant' )
        sys.exit(1)
